<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1">
<meta name="generator" content="pdoc3 0.11.6">
<title>asrpy.asr API documentation</title>
<meta name="description" content="In asrpy.asr you can find the original ASR functions (similar to MATLAB)
as well as a high-level ASR object ready to use with MNE-Python raw data.">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/sanitize.min.css" integrity="sha512-y1dtMcuvtTMJc1yPgEqF0ZjQbhnc/bFhyvIyVNb9Zk5mIGtqVaAB1Ttl28su8AvFMOY0EwRbAe+HCLqj6W7/KA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/13.0.0/typography.min.css" integrity="sha512-Y1DYSb995BAfxobCkKepB1BqJJTPrOp3zPL74AWFugHHmmdcvO+C48WLrUOlhGMc0QG7AE3f7gmvvcrmX2fDoA==" crossorigin>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/default.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:1.5em;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:2em 0 .50em 0}h3{font-size:1.4em;margin:1.6em 0 .7em 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .2s ease-in-out}a:visited{color:#503}a:hover{color:#b62}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900;font-weight:bold}pre code{font-size:.8em;line-height:1.4em;padding:1em;display:block}code{background:#f3f3f3;font-family:"DejaVu Sans Mono",monospace;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source > summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible;min-width:max-content}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em 1em;margin:1em 0}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul ul{padding-left:1em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js" integrity="sha512-D9gUyxqja7hBtkWpPWGt9wfbfaMGVt9gnyCvYa+jojwwPHLCzUm5i8rpk7vD7wNee9bA35eYIjobYPaQuKS1MQ==" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => {
hljs.configure({languages: ['bash', 'css', 'diff', 'graphql', 'ini', 'javascript', 'json', 'plaintext', 'python', 'python-repl', 'rust', 'shell', 'sql', 'typescript', 'xml', 'yaml']});
hljs.highlightAll();
/* Collapse source docstrings */
setTimeout(() => {
[...document.querySelectorAll('.hljs.language-python > .hljs-string')]
.filter(el => el.innerHTML.length > 200 && ['"""', "'''"].includes(el.innerHTML.substring(0, 3)))
.forEach(el => {
let d = document.createElement('details');
d.classList.add('hljs-string');
d.innerHTML = '<summary>"""</summary>' + el.innerHTML.substring(3);
el.replaceWith(d);
});
}, 100);
})</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>asrpy.asr</code></h1>
</header>
<section id="section-intro">
<p>In asrpy.asr you can find the original ASR functions (similar to MATLAB)
as well as a high-level ASR object ready to use with MNE-Python raw data.</p>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="asrpy.asr.asr_calibrate"><code class="name flex">
<span>def <span class="ident">asr_calibrate</span></span>(<span>X,<br>sfreq,<br>cutoff=20,<br>blocksize=100,<br>win_len=0.5,<br>win_overlap=0.66,<br>max_dropout_fraction=0.1,<br>min_clean_fraction=0.25,<br>ab=None,<br>method='euclid')</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def asr_calibrate(X, sfreq, cutoff=20, blocksize=100, win_len=0.5,
                  win_overlap=0.66, max_dropout_fraction=0.1,
                  min_clean_fraction=0.25, ab=None, method=&#39;euclid&#39;):
    &#34;&#34;&#34;Calibration function for the Artifact Subspace Reconstruction method.

    This function can be used if you inted to apply ASR to a simple numpy
    array instead of a mne.io.Raw object. It is equivalent to the MATLAB
    implementation of asr_calibrate (except for some small differences
    introduced by solvers for the eigenspace functions etc).

    The input to this data is a multi-channel time series of calibration data.
    In typical uses the calibration data is clean resting EEG data of ca. 1
    minute duration (can also be longer). One can also use on-task data if the
    fraction of artifact content is below the breakdown point of the robust
    statistics used for estimation (50% theoretical, ~30% practical). If the
    data has a proportion of more than 30-50% artifacts then bad time windows
    should be removed beforehand. This data is used to estimate the thresholds
    that are used by the ASR processing function to identify and remove
    artifact components.

    The calibration data must have been recorded for the same cap design from
    which data for cleanup will be recorded, and ideally should be from the
    same session and same subject, but it is possible to reuse the calibration
    data from a previous session and montage to the extent that the cap is
    placed in the same location (where loss in accuracy is more or less
    proportional to the mismatch in cap placement).

    The calibration data should have been high-pass filtered (for example at
    0.5Hz or 1Hz using a Butterworth IIR filter).

    Parameters
    ----------
    X : array, shape=(n_channels, n_samples)
        *zero-mean* (e.g., high-pass filtered) and reasonably clean EEG of not
        much less than 30 seconds (this method is typically used with 1 minute
        or more).
    sfreq : float
        Sampling rate of the data, in Hz.
    cutoff: float
        Standard deviation cutoff for rejection. X portions whose variance
        is larger than this threshold relative to the calibration data are
        considered missing data and will be removed. Defaults to 20
        (In EEGLab&#39;s `clean_rawdata` the original threshold was set to 5, but
        it is widely recommended to use a value higher than 20).
    blocksize : int
        Block size for calculating the robust data covariance and thresholds,
        in samples; allows to reduce the memory and time requirements of the
        robust estimators by this factor (down to n_chans x n_chans x
        n_samples x 16 / blocksize bytes) (default=100).
    win_len : float
        Window length that is used to check the data for artifact content.
        This is ideally as long as the expected time scale of the artifacts
        but short enough to allow for several 1000 windows to compute
        statistics over (default=0.5).
    win_overlap : float
        Window overlap fraction. The fraction of two successive windows that
        overlaps. Higher overlap ensures that fewer artifact portions are
        going to be missed, but is slower (default=0.66).
    max_dropout_fraction : float
        Maximum fraction of windows that can be subject to signal dropouts
        (e.g., sensor unplugged), used for threshold estimation (default=0.1).
    min_clean_fraction : float
        Minimum fraction of windows that need to be clean, used for threshold
        estimation (default=0.25).
    ab : 2-tuple | None
        Coefficients (A, B) of an IIR filter that is used to shape the
        spectrum of the signal when calculating artifact statistics. The
        output signal does not go through this filter. This is an optional way
        to tune the sensitivity of the algorithm to each frequency component
        of the signal. The default filter is less sensitive at alpha and beta
        frequencies and more sensitive at delta (blinks) and gamma (muscle)
        frequencies. Defaults to None.
    method : {&#39;euclid&#39;, &#39;riemann&#39;}
        Metric to compute the covariance matrix average. For now, only
        euclidean ASR is supported.

    Returns
    -------
    M : array
        Mixing matrix.
    T : array
        Threshold matrix.

    &#34;&#34;&#34;
    if method == &#34;riemann&#34;:
        warnings.warn(&#34;Riemannian ASR is not yet supported. Switching back to&#34;
                      &#34; Euclidean ASR.&#34;)
        method == &#34;euclid&#34;

    logging.debug(&#39;[ASR] Calibrating...&#39;)

    # set number of channels and number of samples
    [nc, ns] = X.shape

    # filter the data
    X, _zf = yulewalk_filter(X, sfreq, ab=ab)

    # window length for calculating thresholds
    N = int(np.round(win_len * sfreq))

    # get block covariances
    U = block_covariance(X, window=blocksize)

    # get geometric median for each block
    # Note: riemann mode is not yet supported, else this could be:
    # Uavg = pyriemann.utils.mean_covariance(U, metric=&#39;riemann&#39;)
    Uavg = geometric_median(U.reshape((-1, nc * nc)) / blocksize)
    Uavg = Uavg.reshape((nc, nc))

    # get the mixing matrix M
    M = linalg.sqrtm(np.real(Uavg))

    # sort the get the sorted eigenvecotors/eigenvalues
    # riemann is not yet supported, else this could be PGA/nonlinear eigenvs
    D, Vtmp = linalg.eigh(M)
    V = Vtmp[:, np.argsort(D)]  # I think numpy sorts them automatically

    # get the threshold matrix T
    x = np.abs(np.dot(V.T, X))
    offsets = np.int_(np.arange(0, ns - N, np.round(N * (1 - win_overlap))))

    # go through all the channels and fit the EEG distribution
    mu = np.zeros(nc)
    sig = np.zeros(nc)
    for ichan in reversed(range(nc)):
        rms = x[ichan, :] ** 2
        Y = []
        for o in offsets:
            Y.append(np.sqrt(np.sum(rms[o:o + N]) / N))
        mu[ichan], sig[ichan], alpha, beta = fit_eeg_distribution(
            Y, min_clean_fraction, max_dropout_fraction)
    T = np.dot(np.diag(mu + cutoff * sig), V.T)

    logging.debug(&#39;[ASR] Calibration done.&#39;)
    return M, T</code></pre>
</details>
<div class="desc"><p>Calibration function for the Artifact Subspace Reconstruction method.</p>
<p>This function can be used if you inted to apply ASR to a simple numpy
array instead of a mne.io.Raw object. It is equivalent to the MATLAB
implementation of asr_calibrate (except for some small differences
introduced by solvers for the eigenspace functions etc).</p>
<p>The input to this data is a multi-channel time series of calibration data.
In typical uses the calibration data is clean resting EEG data of ca. 1
minute duration (can also be longer). One can also use on-task data if the
fraction of artifact content is below the breakdown point of the robust
statistics used for estimation (50% theoretical, ~30% practical). If the
data has a proportion of more than 30-50% artifacts then bad time windows
should be removed beforehand. This data is used to estimate the thresholds
that are used by the ASR processing function to identify and remove
artifact components.</p>
<p>The calibration data must have been recorded for the same cap design from
which data for cleanup will be recorded, and ideally should be from the
same session and same subject, but it is possible to reuse the calibration
data from a previous session and montage to the extent that the cap is
placed in the same location (where loss in accuracy is more or less
proportional to the mismatch in cap placement).</p>
<p>The calibration data should have been high-pass filtered (for example at
0.5Hz or 1Hz using a Butterworth IIR filter).</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>array, shape=(n_channels, n_samples)</code></dt>
<dd><em>zero-mean</em> (e.g., high-pass filtered) and reasonably clean EEG of not
much less than 30 seconds (this method is typically used with 1 minute
or more).</dd>
<dt><strong><code>sfreq</code></strong> :&ensp;<code>float</code></dt>
<dd>Sampling rate of the data, in Hz.</dd>
<dt><strong><code>cutoff</code></strong> :&ensp;<code>float</code></dt>
<dd>Standard deviation cutoff for rejection. X portions whose variance
is larger than this threshold relative to the calibration data are
considered missing data and will be removed. Defaults to 20
(In EEGLab's <code>clean_rawdata</code> the original threshold was set to 5, but
it is widely recommended to use a value higher than 20).</dd>
<dt><strong><code>blocksize</code></strong> :&ensp;<code>int</code></dt>
<dd>Block size for calculating the robust data covariance and thresholds,
in samples; allows to reduce the memory and time requirements of the
robust estimators by this factor (down to n_chans x n_chans x
n_samples x 16 / blocksize bytes) (default=100).</dd>
<dt><strong><code>win_len</code></strong> :&ensp;<code>float</code></dt>
<dd>Window length that is used to check the data for artifact content.
This is ideally as long as the expected time scale of the artifacts
but short enough to allow for several 1000 windows to compute
statistics over (default=0.5).</dd>
<dt><strong><code>win_overlap</code></strong> :&ensp;<code>float</code></dt>
<dd>Window overlap fraction. The fraction of two successive windows that
overlaps. Higher overlap ensures that fewer artifact portions are
going to be missed, but is slower (default=0.66).</dd>
<dt><strong><code>max_dropout_fraction</code></strong> :&ensp;<code>float</code></dt>
<dd>Maximum fraction of windows that can be subject to signal dropouts
(e.g., sensor unplugged), used for threshold estimation (default=0.1).</dd>
<dt><strong><code>min_clean_fraction</code></strong> :&ensp;<code>float</code></dt>
<dd>Minimum fraction of windows that need to be clean, used for threshold
estimation (default=0.25).</dd>
<dt><strong><code>ab</code></strong> :&ensp;<code>2-tuple | None</code></dt>
<dd>Coefficients (A, B) of an IIR filter that is used to shape the
spectrum of the signal when calculating artifact statistics. The
output signal does not go through this filter. This is an optional way
to tune the sensitivity of the algorithm to each frequency component
of the signal. The default filter is less sensitive at alpha and beta
frequencies and more sensitive at delta (blinks) and gamma (muscle)
frequencies. Defaults to None.</dd>
<dt><strong><code>method</code></strong> :&ensp;<code>{'euclid', 'riemann'}</code></dt>
<dd>Metric to compute the covariance matrix average. For now, only
euclidean ASR is supported.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>M</code></strong> :&ensp;<code>array</code></dt>
<dd>Mixing matrix.</dd>
<dt><strong><code>T</code></strong> :&ensp;<code>array</code></dt>
<dd>Threshold matrix.</dd>
</dl></div>
</dd>
<dt id="asrpy.asr.asr_process"><code class="name flex">
<span>def <span class="ident">asr_process</span></span>(<span>data,<br>sfreq,<br>M,<br>T,<br>windowlen=0.5,<br>lookahead=0.25,<br>stepsize=32,<br>maxdims=0.66,<br>ab=None,<br>R=None,<br>Zi=None,<br>cov=None,<br>carry=None,<br>return_states=False,<br>method='euclid',<br>mem_splits=3)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def asr_process(data, sfreq, M, T, windowlen=0.5, lookahead=0.25, stepsize=32,
                maxdims=0.66, ab=None, R=None, Zi=None, cov=None, carry=None,
                return_states=False, method=&#34;euclid&#34;, mem_splits=3):
    &#34;&#34;&#34;Apply the Artifact Subspace Reconstruction method to a data array.

    This function is used to clean multi-channel signal using the ASR method.
    The required inputs are the data matrix and the sampling rate of the data.

    `asr_process` can be used if you inted to apply ASR to a simple numpy
    array instead of a mne.io.Raw object. It is equivalent to the MATLAB
    implementation of `asr_process` (except for some small differences
    introduced by solvers for the eigenspace functions etc).

    Parameters
    ----------
    data : array, shape=(n_channels, n_samples)
        Raw data.
    sfreq : float
        The sampling rate of the data.
    M : array, shape=(n_channels, n_channels)
        The Mixing matrix (as fitted with asr_calibrate).
    T : array, shape=(n_channels, n_channels)
        The Threshold matrix (as fitted with asr_calibrate).
    windowlen : float
        Window length that is used to check the data for artifact content.
        This is ideally as long as the expected time scale of the artifacts
        but short enough to allow for several 1000 windows to compute
        statistics over (default=0.5).
    lookahead:
        Amount of look-ahead that the algorithm should use. Since the
        processing is causal, the output signal will be delayed by this
        amount. This value is in seconds and should be between 0 (no
        lookahead) and WindowLength/2 (optimal lookahead). The recommended
        value is WindowLength/2. Default: 0.25
    stepsize:
        The steps in which the algorithm will be updated. The larger this is,
        the faster the algorithm will be. The value must not be larger than
        WindowLength * SamplingRate. The minimum value is 1 (update for every
        sample) while a good value would be sfreq//3. Note that an update
        is always performed also on the first and last sample of the data
        chunk. Default: 32
    max_dims : float, int
        Maximum dimensionality of artifacts to remove. This parameter
        denotes the maximum number of dimensions which can be removed from
        each segment. If larger than 1, `int(max_dims)` will denote the
        maximum number of dimensions removed from the data. If smaller than 1,
        `max_dims` describes a fraction of total dimensions. Defaults to 0.66.
    ab : 2-tuple | None
        Coefficients (A, B) of an IIR filter that is used to shape the
        spectrum of the signal when calculating artifact statistics. The
        output signal does not go through this filter. This is an optional way
        to tune the sensitivity of the algorithm to each frequency component
        of the signal. The default filter is less sensitive at alpha and beta
        frequencies and more sensitive at delta (blinks) and gamma (muscle)
        frequencies. Defaults to None.
    R : array, shape=(n_channels, n_channels)
        Previous reconstruction matrix. Defaults to None.
    Zi : array
        Previous filter conditions. Defaults to None.
    cov : array, shape=([n_trials, ]n_channels, n_channels) | None
        Covariance. If None (default), then it is computed from ``X_filt``.
        If a 3D array is provided, the average covariance is computed from
        all the elements in it. Defaults to None.
    carry :
        Initial portion of the data that will be added to the current data.
        If None, data will be interpolated. Defaults to None.
    return_states : bool
        If True, returns a dict including the updated states {&#34;M&#34;:M, &#34;T&#34;:T,
        &#34;R&#34;:R, &#34;Zi&#34;:Zi, &#34;cov&#34;:cov, &#34;carry&#34;:carry}. Defaults to False.
    method : {&#39;euclid&#39;, &#39;riemann&#39;}
        Metric to compute the covariance matrix average. Currently, only
        euclidean ASR is supported.
    mem_splits : int
        Split the array in `mem_splits` segments to save memory.


    Returns
    -------
    clean : array, shape=(n_channels, n_samples)
        Clean data.
    state : dict
        Output ASR parameters {&#34;M&#34;:M, &#34;T&#34;:T, &#34;R&#34;:R, &#34;Zi&#34;:Zi, &#34;cov&#34;:cov,
        &#34;carry&#34;:carry}.

    &#34;&#34;&#34;
    if method == &#34;riemann&#34;:
        warnings.warn(&#34;Riemannian ASR is not yet supported. Switching back to&#34;
                      &#34; Euclidean ASR.&#34;)
        method == &#34;euclid&#34;

    # calculate the the actual max dims based on the fraction parameter
    if maxdims &lt; 1:
        maxdims = np.round(len(data) * maxdims)

    # set initial filter conditions of none was passed
    if Zi is None:
        _, Zi = yulewalk_filter(data, ab=ab, sfreq=sfreq,
                                zi=np.ones([len(data), 8]))

    # set the number of channels
    C, S = data.shape

    # set the number of windows
    N = np.round(windowlen * sfreq).astype(int)
    P = np.round(lookahead * sfreq).astype(int)

    # interpolate a portion of the data if no buffer was given
    if carry is None:
        carry = np.tile(2 * data[:, 0],
                        (P, 1)).T - data[:, np.mod(np.arange(P, 0, -1), S)]
    data = np.concatenate([carry, data], axis=-1)

    # splits = np.ceil(C*C*S*8*8 + C*C*8*s/stepsize + C*S*8*2 + S*8*5)...
    splits = mem_splits  # TODO: make param and use for parallelization

    # loop over smaller segments of the data (for memory purposes)
    last_trivial = False
    last_R = None
    for i in range(splits):

        # set the current range
        i_range = np.arange(i * S // splits,
                            np.min([(i + 1) * S // splits, S]),
                            dtype=int)

        # filter the current window with yule-walker
        X, Zi = yulewalk_filter(data[:, i_range + P], sfreq=sfreq,
                                zi=Zi, ab=ab, axis=-1)

        # compute a moving average covariance
        Xcov, cov = \
            ma_filter(N,
                      np.reshape(np.multiply(np.reshape(X, (1, C, -1)),
                                             np.reshape(X, (C, 1, -1))),
                                 (C * C, -1)), cov)

        # set indices at which we update the signal
        update_at = np.arange(stepsize,
                              Xcov.shape[-1] + stepsize - 2,
                              stepsize)
        update_at = np.minimum(update_at, Xcov.shape[-1]) - 1

        # set the previous reconstruction matrix if none was assigned
        if last_R is None:
            update_at = np.concatenate([[0], update_at])
            last_R = np.eye(C)

        Xcov = np.reshape(Xcov[:, update_at], (C, C, -1))

        # loop through the updating intervals
        last_n = 0
        for j in range(len(update_at) - 1):

            # get the eigenvectors/values.For method &#39;riemann&#39;, this should
            # be replaced with PGA/ nonlinear eigenvalues
            D, V = np.linalg.eigh(Xcov[:, :, j])

            # determine which components to keep
            keep = np.logical_or(D &lt; np.sum((T @ V)**2, axis=0),
                                 np.arange(C) + 1 &lt; (C - maxdims))
            trivial = np.all(keep)

            # set the reconstruction matrix (ie. reconstructing artifact
            # components using the mixing matrix)
            if not trivial:
                inv = pinv(np.multiply(keep[:, np.newaxis], V.T @ M))
                R = np.real(M @ inv @ V.T)
            else:
                R = np.eye(C)

            # apply the reconstruction
            n = update_at[j] + 1
            if (not trivial) or (not last_trivial):

                subrange = i_range[np.arange(last_n, n)]

                # generate a cosine signal
                blend_x = np.pi * np.arange(1, n - last_n + 1) / (n - last_n)
                blend = (1 - np.cos(blend_x)) / 2

                # use cosine blending to replace data with reconstructed data
                tmp_data = data[:, subrange]
                data[:, subrange] = np.multiply(blend, R @ tmp_data) + \
                                    np.multiply(1 - blend, last_R @ tmp_data) # noqa

            # set the parameters for the next iteration
            last_n, last_R, last_trivial = n, R, trivial

    # assign a new lookahead portion
    carry = np.concatenate([carry, data[:, -P:]])
    carry = carry[:, -P:]

    if return_states:
        return data[:, :-P], {&#34;M&#34;: M, &#34;T&#34;: T, &#34;R&#34;: R, &#34;Zi&#34;: Zi,
                              &#34;cov&#34;: cov, &#34;carry&#34;: carry}
    else:
        return data[:, :-P]</code></pre>
</details>
<div class="desc"><p>Apply the Artifact Subspace Reconstruction method to a data array.</p>
<p>This function is used to clean multi-channel signal using the ASR method.
The required inputs are the data matrix and the sampling rate of the data.</p>
<p><code><a title="asrpy.asr.asr_process" href="#asrpy.asr.asr_process">asr_process()</a></code> can be used if you inted to apply ASR to a simple numpy
array instead of a mne.io.Raw object. It is equivalent to the MATLAB
implementation of <code><a title="asrpy.asr.asr_process" href="#asrpy.asr.asr_process">asr_process()</a></code> (except for some small differences
introduced by solvers for the eigenspace functions etc).</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>data</code></strong> :&ensp;<code>array, shape=(n_channels, n_samples)</code></dt>
<dd>Raw data.</dd>
<dt><strong><code>sfreq</code></strong> :&ensp;<code>float</code></dt>
<dd>The sampling rate of the data.</dd>
<dt><strong><code>M</code></strong> :&ensp;<code>array, shape=(n_channels, n_channels)</code></dt>
<dd>The Mixing matrix (as fitted with asr_calibrate).</dd>
<dt><strong><code>T</code></strong> :&ensp;<code>array, shape=(n_channels, n_channels)</code></dt>
<dd>The Threshold matrix (as fitted with asr_calibrate).</dd>
<dt><strong><code>windowlen</code></strong> :&ensp;<code>float</code></dt>
<dd>Window length that is used to check the data for artifact content.
This is ideally as long as the expected time scale of the artifacts
but short enough to allow for several 1000 windows to compute
statistics over (default=0.5).</dd>
<dt>lookahead:</dt>
<dt>Amount of look-ahead that the algorithm should use. Since the</dt>
<dt>processing is causal, the output signal will be delayed by this</dt>
<dt>amount. This value is in seconds and should be between 0 (no</dt>
<dt>lookahead) and WindowLength/2 (optimal lookahead). The recommended</dt>
<dt>value is WindowLength/2. Default: 0.25</dt>
<dt>stepsize:</dt>
<dt>The steps in which the algorithm will be updated. The larger this is,</dt>
<dt>the faster the algorithm will be. The value must not be larger than</dt>
<dt>WindowLength * SamplingRate. The minimum value is 1 (update for every</dt>
<dt>sample) while a good value would be sfreq//3. Note that an update</dt>
<dt>is always performed also on the first and last sample of the data</dt>
<dt>chunk. Default: 32</dt>
<dt><strong><code>max_dims</code></strong> :&ensp;<code>float, int</code></dt>
<dd>Maximum dimensionality of artifacts to remove. This parameter
denotes the maximum number of dimensions which can be removed from
each segment. If larger than 1, <code>int(max_dims)</code> will denote the
maximum number of dimensions removed from the data. If smaller than 1,
<code>max_dims</code> describes a fraction of total dimensions. Defaults to 0.66.</dd>
<dt><strong><code>ab</code></strong> :&ensp;<code>2-tuple | None</code></dt>
<dd>Coefficients (A, B) of an IIR filter that is used to shape the
spectrum of the signal when calculating artifact statistics. The
output signal does not go through this filter. This is an optional way
to tune the sensitivity of the algorithm to each frequency component
of the signal. The default filter is less sensitive at alpha and beta
frequencies and more sensitive at delta (blinks) and gamma (muscle)
frequencies. Defaults to None.</dd>
<dt><strong><code>R</code></strong> :&ensp;<code>array, shape=(n_channels, n_channels)</code></dt>
<dd>Previous reconstruction matrix. Defaults to None.</dd>
<dt><strong><code>Zi</code></strong> :&ensp;<code>array</code></dt>
<dd>Previous filter conditions. Defaults to None.</dd>
<dt><strong><code>cov</code></strong> :&ensp;<code>array, shape=([n_trials, ]n_channels, n_channels) | None</code></dt>
<dd>Covariance. If None (default), then it is computed from <code>X_filt</code>.
If a 3D array is provided, the average covariance is computed from
all the elements in it. Defaults to None.</dd>
<dt>carry :</dt>
<dt>Initial portion of the data that will be added to the current data.</dt>
<dt>If None, data will be interpolated. Defaults to None.</dt>
<dt><strong><code>return_states</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, returns a dict including the updated states {"M":M, "T":T,
"R":R, "Zi":Zi, "cov":cov, "carry":carry}. Defaults to False.</dd>
<dt><strong><code>method</code></strong> :&ensp;<code>{'euclid', 'riemann'}</code></dt>
<dd>Metric to compute the covariance matrix average. Currently, only
euclidean ASR is supported.</dd>
<dt><strong><code>mem_splits</code></strong> :&ensp;<code>int</code></dt>
<dd>Split the array in <code>mem_splits</code> segments to save memory.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>clean</code></strong> :&ensp;<code>array, shape=(n_channels, n_samples)</code></dt>
<dd>Clean data.</dd>
<dt><strong><code>state</code></strong> :&ensp;<code>dict</code></dt>
<dd>Output ASR parameters {"M":M, "T":T, "R":R, "Zi":Zi, "cov":cov,
"carry":carry}.</dd>
</dl></div>
</dd>
<dt id="asrpy.asr.clean_windows"><code class="name flex">
<span>def <span class="ident">clean_windows</span></span>(<span>X,<br>sfreq,<br>max_bad_chans=0.2,<br>zthresholds=[-3.5, 5],<br>win_len=0.5,<br>win_overlap=0.66,<br>min_clean_fraction=0.25,<br>max_dropout_fraction=0.1)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def clean_windows(X, sfreq, max_bad_chans=0.2, zthresholds=[-3.5, 5],
                  win_len=.5, win_overlap=0.66, min_clean_fraction=0.25,
                  max_dropout_fraction=0.1):
    &#34;&#34;&#34;Remove periods with abnormally high-power content from continuous data.

    This function cuts segments from the data which contain high-power
    artifacts. Specifically, only windows are retained which have less than a
    certain fraction of &#34;bad&#34; channels, where a channel is bad in a window if
    its power is above or below a given upper/lower threshold (in standard
    deviations from a robust estimate of the EEG power distribution in the
    channel).

    Parameters
    ----------
    X : array, shape=(n_channels, n_samples)
        Continuous data set, assumed to be appropriately high-passed (e.g. &gt;
        1Hz or 0.5Hz - 2.0Hz transition band)
    max_bad_chans : float
        The maximum number or fraction of bad channels that a retained window
        may still contain (more than this and it is removed). Reasonable range
        is 0.05 (very clean output) to 0.3 (very lax cleaning of only coarse
        artifacts) (default=0.2).
    zthresholds : 2-tuple
        The minimum and maximum standard deviations within which the power of
        a channel must lie (relative to a robust estimate of the clean EEG
        power distribution in the channel) for it to be considered &#34;not bad&#34;.
        (default=[-3.5, 5]).

    The following are detail parameters that usually do not have to be tuned.
    If you can&#39;t get the function to do what you want, you might consider
    adapting these to your data.

    win_len : float
        Window length that is used to check the data for artifact content.
        This is ideally as long as the expected time scale of the artifacts
        but not shorter than half a cycle of the high-pass filter that was
        used. Default: 1.
    win_overlap : float
        Window overlap fraction. The fraction of two successive windows that
        overlaps. Higher overlap ensures that fewer artifact portions are
        going to be missed, but is slower (default=0.66).
    min_clean_fraction : float
        Minimum fraction that needs to be clean. This is the minimum fraction
        of time windows that need to contain essentially uncontaminated EEG.
        (default=0.25)
    max_dropout_fraction : float
        Maximum fraction that can have dropouts. This is the maximum fraction
        of time windows that may have arbitrarily low amplitude (e.g., due to
        the sensors being unplugged) (default=0.1).

    Returns
    -------
    clean : array, shape=(n_channels, n_samples)
        Dataset with bad time periods removed.
    sample_mask : boolean array, shape=(1, n_samples)
        Mask of retained samples (logical array).

    &#34;&#34;&#34;
    assert 0 &lt; max_bad_chans &lt; 1, &#34;max_bad_chans must be a fraction !&#34;

    # set internal variables
    truncate_quant = [0.0220, 0.6000]
    step_sizes = [0.01, 0.01]
    shape_range = np.arange(1.7, 3.5, 0.15)
    max_bad_chans = np.round(X.shape[0] * max_bad_chans)

    # set data indices
    [nc, ns] = X.shape
    N = int(win_len * sfreq)
    offsets = np.int_(np.round(np.arange(0, ns - N, (N * (1 - win_overlap)))))
    logging.debug(&#39;[ASR] Determining channel-wise rejection thresholds&#39;)

    wz = np.zeros((nc, len(offsets)))
    for ichan in range(nc):

        # compute root mean squared amplitude
        x = X[ichan, :] ** 2
        Y = np.array([np.sqrt(np.sum(x[o:o + N]) / N) for o in offsets])

        # fit a distribution to the clean EEG part
        mu, sig, alpha, beta = fit_eeg_distribution(
            Y, min_clean_fraction, max_dropout_fraction, truncate_quant,
            step_sizes, shape_range)
        # calculate z scores
        wz[ichan] = (Y - mu) / sig

    # sort z scores into quantiles
    wz[np.isnan(wz)] = np.inf  # Nan to inf
    swz = np.sort(wz, axis=0)

    # determine which windows to remove
    if np.max(zthresholds) &gt; 0:
        mask1 = swz[-(int(max_bad_chans) + 1), :] &gt; np.max(zthresholds)
    if np.min(zthresholds) &lt; 0:
        mask2 = (swz[1 + int(max_bad_chans - 1), :] &lt; np.min(zthresholds))

    # combine the two thresholds
    remove_mask = np.logical_or.reduce((mask1, mask2))
    removed_wins = np.where(remove_mask)

    # reconstruct the samples to remove
    sample_maskidx = []
    for i in range(len(removed_wins[0])):
        if i == 0:
            sample_maskidx = np.arange(
                offsets[removed_wins[0][i]], offsets[removed_wins[0][i]] + N)
        else:
            sample_maskidx = np.vstack((
                sample_maskidx,
                np.arange(offsets[removed_wins[0][i]],
                          offsets[removed_wins[0][i]] + N)
            ))

    # delete the bad chunks from the data
    sample_mask2remove = np.unique(sample_maskidx)
    if sample_mask2remove.size:
        clean = np.delete(X, sample_mask2remove, 1)
        sample_mask = np.ones((1, ns), dtype=bool)
        sample_mask[0, sample_mask2remove] = False
    else:
        sample_mask = np.ones((1, ns), dtype=bool)
        clean = X
        print(&#39;Try calibrating ASR with cleaner data.&#39;)

    return clean, sample_mask</code></pre>
</details>
<div class="desc"><p>Remove periods with abnormally high-power content from continuous data.</p>
<p>This function cuts segments from the data which contain high-power
artifacts. Specifically, only windows are retained which have less than a
certain fraction of "bad" channels, where a channel is bad in a window if
its power is above or below a given upper/lower threshold (in standard
deviations from a robust estimate of the EEG power distribution in the
channel).</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>array, shape=(n_channels, n_samples)</code></dt>
<dd>Continuous data set, assumed to be appropriately high-passed (e.g. &gt;
1Hz or 0.5Hz - 2.0Hz transition band)</dd>
<dt><strong><code>max_bad_chans</code></strong> :&ensp;<code>float</code></dt>
<dd>The maximum number or fraction of bad channels that a retained window
may still contain (more than this and it is removed). Reasonable range
is 0.05 (very clean output) to 0.3 (very lax cleaning of only coarse
artifacts) (default=0.2).</dd>
<dt><strong><code>zthresholds</code></strong> :&ensp;<code>2-tuple</code></dt>
<dd>The minimum and maximum standard deviations within which the power of
a channel must lie (relative to a robust estimate of the clean EEG
power distribution in the channel) for it to be considered "not bad".
(default=[-3.5, 5]).</dd>
</dl>
<p>The following are detail parameters that usually do not have to be tuned.
If you can't get the function to do what you want, you might consider
adapting these to your data.</p>
<dl>
<dt><strong><code>win_len</code></strong> :&ensp;<code>float</code></dt>
<dd>Window length that is used to check the data for artifact content.
This is ideally as long as the expected time scale of the artifacts
but not shorter than half a cycle of the high-pass filter that was
used. Default: 1.</dd>
<dt><strong><code>win_overlap</code></strong> :&ensp;<code>float</code></dt>
<dd>Window overlap fraction. The fraction of two successive windows that
overlaps. Higher overlap ensures that fewer artifact portions are
going to be missed, but is slower (default=0.66).</dd>
<dt><strong><code>min_clean_fraction</code></strong> :&ensp;<code>float</code></dt>
<dd>Minimum fraction that needs to be clean. This is the minimum fraction
of time windows that need to contain essentially uncontaminated EEG.
(default=0.25)</dd>
<dt><strong><code>max_dropout_fraction</code></strong> :&ensp;<code>float</code></dt>
<dd>Maximum fraction that can have dropouts. This is the maximum fraction
of time windows that may have arbitrarily low amplitude (e.g., due to
the sensors being unplugged) (default=0.1).</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>clean</code></strong> :&ensp;<code>array, shape=(n_channels, n_samples)</code></dt>
<dd>Dataset with bad time periods removed.</dd>
<dt><strong><code>sample_mask</code></strong> :&ensp;<code>boolean array, shape=(1, n_samples)</code></dt>
<dd>Mask of retained samples (logical array).</dd>
</dl></div>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="asrpy.asr.ASR"><code class="flex name class">
<span>class <span class="ident">ASR</span></span>
<span>(</span><span>sfreq,<br>cutoff=20,<br>blocksize=100,<br>win_len=0.5,<br>win_overlap=0.66,<br>max_dropout_fraction=0.1,<br>min_clean_fraction=0.25,<br>ab=None,<br>max_bad_chans=0.1,<br>method='euclid')</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ASR():
    &#34;&#34;&#34;Artifact Subspace Reconstruction.

    Artifact subspace reconstruction (ASR) is an automated, online,
    component-based artifact removal method for removing transient or
    large-amplitude artifacts in multi-channel EEG recordings [1]_.

    Parameters
    ----------
    sfreq : float
        Sampling rate of the data, in Hz.
    cutoff: float
        Standard deviation cutoff for rejection. X portions whose variance
        is larger than this threshold relative to the calibration data are
        considered missing data and will be removed. The most aggressive value
        that can be used without losing too much EEG is 2.5. Recommended to
        use with more conservative values ranging from 20 - 30.
        Defaults to 20.
    blocksize : int
        Block size for calculating the robust data covariance and thresholds,
        in samples; allows to reduce the memory and time requirements of the
        robust estimators by this factor (down to Channels x Channels x Samples
        x 16 / Blocksize bytes) (default=100).
    win_len : float
        Window length (s) that is used to check the data for artifact content.
        This is ideally as long as the expected time scale of the artifacts but
        not shorter than half a cycle of the high-pass filter that was used
        (default=0.5).
    win_overlap : float
        Window overlap fraction. The fraction of two successive windows that
        overlaps. Higher overlap ensures that fewer artifact portions are going
        to be missed, but is slower (default=0.66).
    max_dropout_fraction : float
        Maximum fraction of windows that can be subject to signal dropouts
        (e.g., sensor unplugged), used for threshold estimation (default=0.1).
    min_clean_fraction : float
        Minimum fraction of windows that need to be clean, used for threshold
        estimation (default=0.25).
    ab : 2-tuple | None
        Coefficients (A, B) of an IIR filter that is used to shape the
        spectrum of the signal when calculating artifact statistics. The
        output signal does not go through this filter. This is an optional way
        to tune the sensitivity of the algorithm to each frequency component
        of the signal. The default filter is less sensitive at alpha and beta
        frequencies and more sensitive at delta (blinks) and gamma (muscle)
        frequencies. Defaults to None.
    max_bad_chans : float
        The maximum number or fraction of bad channels that a retained window
        may still contain (more than this and it is removed). Reasonable range
        is 0.05 (very clean output) to 0.3 (very lax cleaning of only coarse
        artifacts) (default=0.2).
    method : {&#39;riemann&#39;, &#39;euclid&#39;}
        Method to use. If riemann, use the riemannian-modified version of
        ASR [2]_. Currently, only euclidean ASR is supported. Defaults to
        &#34;euclid&#34;.

    Attributes
    ----------
    sfreq: array, shape=(n_channels, filter_order)
        Filter initial conditions.
    cutoff: float
        Standard deviation cutoff for rejection.
    blocksize : int
        Block size for calculating the robust data covariance and thresholds.
    win_len : float
        Window length (s) that is used to check the data for artifact content.
    win_overlap : float
        Window overlap fraction.
    max_dropout_fraction : float
        Maximum fraction of windows that can be subject to signal dropouts.
    min_clean_fraction : float
        Minimum fraction of windows.
    max_bad_chans : float
        The maximum fraction of bad channels.
    method : {&#39;riemann&#39;, &#39;euclid&#39;}
        Method to use.
    A, B: arrays
        Coefficients of an IIR filter that is used to shape the spectrum of the
        signal when calculating artifact statistics. The output signal does not
        go through this filter. This is an optional way to tune the sensitivity
        of the algorithm to each frequency component of the signal. The default
        filter is less sensitive at alpha and beta frequencies and more
        sensitive at delta (blinks) and gamma (muscle) frequencies.
    M : array, shape=(channels, channels)
        The mixing matrix to fit ASR data.
    T : array, shape=(channels, channels)
        The mixing matrix to fit ASR data.

    References
    ----------
    .. [1] Kothe, C. A. E., &amp; Jung, T. P. (2016). U.S. Patent Application No.
       14/895,440. https://patents.google.com/patent/US20160113587A1/en
    .. [2] Blum, S., Jacobsen, N. S. J., Bleichner, M. G., &amp; Debener, S.
       (2019). A Riemannian Modification of Artifact Subspace Reconstruction
       for EEG Artifact Handling. Frontiers in Human Neuroscience, 13.
       https://doi.org/10.3389/fnhum.2019.00141

    &#34;&#34;&#34;

    def __init__(self, sfreq, cutoff=20, blocksize=100, win_len=0.5,
                 win_overlap=0.66, max_dropout_fraction=0.1,
                 min_clean_fraction=0.25, ab=None, max_bad_chans=0.1,
                 method=&#34;euclid&#34;):

        # set attributes
        self.sfreq = sfreq
        self.cutoff = cutoff
        self.blocksize = blocksize
        self.win_len = win_len
        self.win_overlap = win_overlap
        self.max_dropout_fraction = max_dropout_fraction
        self.min_clean_fraction = min_clean_fraction
        self.max_bad_chans = max_bad_chans
        self.method = &#34;euclid&#34;  # NOTE: riemann is not yet available
        self._fitted = False

        # set default yule-walker filter
        if ab is None:
            yw_f = np.array([0, 2, 3, 13, 16, 40,
                             np.minimum(80.0, (self.sfreq / 2.0) - 1.0),
                             self.sfreq / 2.0]) * 2.0 / self.sfreq
            yw_m = np.array([3, 0.75, 0.33, 0.33, 1, 1, 3, 3])
            self.B, self.A = yulewalk(8, yw_f, yw_m)
        else:
            self.A, self.B = ab

        self._reset()

    def _reset(self):
        &#34;&#34;&#34;Reset state variables.&#34;&#34;&#34;
        self.M = None
        self.T = None

        # TODO: The following parameters are effectively not used. Still,
        #  they can be set manually via asr.transform(return_states=True)
        self.R = None
        self.carry = None
        self.Zi = None
        self.cov = None
        self._fitted = False

    def fit(self, raw, picks=&#34;eeg&#34;, start=0, stop=None,
            return_clean_window=False):
        &#34;&#34;&#34;Calibration for the Artifact Subspace Reconstruction method.

        The input to this data is a multi-channel time series of calibration
        data. In typical uses the calibration data is clean resting EEG data
        of data if the fraction of artifact content is below the breakdown
        point of the robust statistics used for estimation (50% theoretical,
        ~30% practical). If the data has a proportion of more than 30-50%
        artifacts then bad time windows should be removed beforehand. This
        data is used to estimate the thresholds that are used by the ASR
        processing function to identify and remove artifact components.

        The calibration data must have been recorded for the same cap design
        from which data for cleanup will be recorded, and ideally should be
        from the same session and same subject, but it is possible to reuse
        the calibration data from a previous session and montage to the
        extent that the cap is placed in the same location (where loss in
        accuracy is more or less proportional to the mismatch in cap
        placement).

        Parameters
        ----------
        raw : instance of mne.io.Raw
            Instance of mne.io.Raw to be used for fitting the ASR.
            The calibration data should have been high-pass filtered (for
            example at 0.5Hz or 1Hz using a Butterworth IIR filter), and be
            reasonably clean not less than 30 seconds (this method is
            typically used with 1 minute or more).
        picks : str | list | slice | None
            Channels used to fit the ASR. All channels should be of the same
            type (e.g. &#34;eeg&#34;, &#34;grads&#34;). Slices and lists of integers will
            be interpreted as channel indices. In lists, channel
            name strings (e.g., [&#39;MEG0111&#39;, &#39;MEG2623&#39;] will pick the given
            channels. Note that channels in info[&#39;bads&#39;] will be included if
            their names or indices are explicitly provided. Defaults to &#34;eeg&#34;.
        start : int
            The first sample to use for fitting the data. Defaults to 0.
        stop : int | None
            The last sample to use for fitting the data. If `None`, all
            samples after `start` will be used for fitting. Defaults to None.
        return_clean_window : Bool
            If True, the method will return the variables `clean` (the cropped
             dataset which was used to fit the ASR) and `sample_mask` (a
             logical mask of which samples were included/excluded from fitting
             the ASR). Defaults to False.

        Returns
        -------
        clean : array, shape=(n_channels, n_samples)
            The cropped version of the dataset which was used to calibrate
            the ASR. This array is a result of the `clean_windows` function
            and no ASR was applied to it.
        sample_mask : boolean array, shape=(1, n_samples)
            Logical mask of the samples which were used to train the ASR.

        &#34;&#34;&#34;

        # extract the data
        X = raw.get_data(picks=picks, start=start, stop=stop)

        # Find artifact-free windows first
        clean, sample_mask = clean_windows(
            X,
            sfreq=self.sfreq,
            win_len=self.win_len,
            win_overlap=self.win_overlap,
            max_bad_chans=self.max_bad_chans,
            min_clean_fraction=self.min_clean_fraction,
            max_dropout_fraction=self.max_dropout_fraction)

        # Perform calibration
        self.M, self.T = asr_calibrate(
            clean,
            sfreq=self.sfreq,
            cutoff=self.cutoff,
            blocksize=self.blocksize,
            win_len=self.win_len,
            win_overlap=self.win_overlap,
            max_dropout_fraction=self.max_dropout_fraction,
            min_clean_fraction=self.min_clean_fraction,
            ab=(self.A, self.B),
            method=self.method)

        self._fitted = True

        # return data if required
        if return_clean_window:
            return clean, sample_mask

    def transform(self, raw, picks=&#34;eeg&#34;, lookahead=0.25, stepsize=32,
                  maxdims=0.66, return_states=False, mem_splits=3):
        &#34;&#34;&#34;Apply Artifact Subspace Reconstruction.

        Parameters
        ----------
        raw : instance of mne.io.Raw
            Instance of mne.io.Raw to be transformed by the ASR.
        picks : str | list | slice | None
            Channels to be transformed by the ASR. Should be the same set of
            channels as used by `ASR.fit()`. All channels should be of the
            same type (e.g. &#34;eeg&#34;, &#34;grads&#34;). Slices and lists of integers will
            be interpreted as channel indices. In lists, channel
            name strings (e.g., [&#39;MEG0111&#39;, &#39;MEG2623&#39;] will pick the given
            channels. Note that channels in info[&#39;bads&#39;] will be included if
            their names or indices are explicitly provided. Defaults to &#34;eeg&#34;.
        lookahead : float
            Amount of look-ahead that the algorithm should use (in seconds).
            This value should be between 0 (no lookahead) and WindowLength/2
            (optimal lookahead). The recommended value is WindowLength/2.
            Default: 0.25

        Note: Other than in `asr_process`, the signal will be readjusted
            to eliminate any temporal jitter and automatically readjust it to
            the correct time points. Zero-padding will be applied to the last
            `lookahead` portion of the data, possibly resulting in inaccuracies
            for the final `lookahead` seconds of the recording.
        stepsize : int
            The steps in which the algorithm will be updated. The larger this
            is, the faster the algorithm will be. The value must not be larger
            than WindowLength * SamplingRate. The minimum value is 1 (update
            for every sample) while a good value would be sfreq//3. Note that
            an update is always performed also on the first and last sample of
            the data chunk. Default: 32
        max_dims : float, int
            Maximum dimensionality of artifacts to remove. This parameter
            denotes the maximum number of dimensions which can be removed from
            each segment. If larger than 1, `int(max_dims)` will denote the
            maximum number of dimensions removed from the data. If smaller
            than 1, `max_dims` describes a fraction of total dimensions.
            Defaults to 0.66.
        return_states : bool
            If True, returns a dict including the updated states {&#34;M&#34;:M,
            &#34;T&#34;:T, &#34;R&#34;:R, &#34;Zi&#34;:Zi, &#34;cov&#34;:cov, &#34;carry&#34;:carry}. Defaults to
            False.
        mem_splits : int
            Split the array in `mem_splits` segments to save memory.

        Returns
        -------
        out : array, shape=(n_channels, n_samples)
            Filtered data.

        &#34;&#34;&#34;
        # extract the data
        X = raw.get_data(picks=picks)

        # add lookahead padding at the end
        lookahead_samples = int(self.sfreq * lookahead)
        X = np.concatenate([X,
                            np.zeros([X.shape[0], lookahead_samples])],
                           axis=1)

        # apply ASR
        X = asr_process(X, self.sfreq, self.M, self.T, self.win_len,
                        lookahead, stepsize, maxdims, (self.A, self.B),
                        self.R, self.Zi, self.cov, self.carry,
                        return_states, self.method, mem_splits)

        # remove lookahead portion from start
        X = X[:, lookahead_samples:]

        # Return a modifier raw instance
        raw = raw.copy()
        raw.apply_function(lambda x: X, picks=picks,
                           channel_wise=False)
        return raw</code></pre>
</details>
<div class="desc"><p>Artifact Subspace Reconstruction.</p>
<p>Artifact subspace reconstruction (ASR) is an automated, online,
component-based artifact removal method for removing transient or
large-amplitude artifacts in multi-channel EEG recordings [1]_.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>sfreq</code></strong> :&ensp;<code>float</code></dt>
<dd>Sampling rate of the data, in Hz.</dd>
<dt><strong><code>cutoff</code></strong> :&ensp;<code>float</code></dt>
<dd>Standard deviation cutoff for rejection. X portions whose variance
is larger than this threshold relative to the calibration data are
considered missing data and will be removed. The most aggressive value
that can be used without losing too much EEG is 2.5. Recommended to
use with more conservative values ranging from 20 - 30.
Defaults to 20.</dd>
<dt><strong><code>blocksize</code></strong> :&ensp;<code>int</code></dt>
<dd>Block size for calculating the robust data covariance and thresholds,
in samples; allows to reduce the memory and time requirements of the
robust estimators by this factor (down to Channels x Channels x Samples
x 16 / Blocksize bytes) (default=100).</dd>
<dt><strong><code>win_len</code></strong> :&ensp;<code>float</code></dt>
<dd>Window length (s) that is used to check the data for artifact content.
This is ideally as long as the expected time scale of the artifacts but
not shorter than half a cycle of the high-pass filter that was used
(default=0.5).</dd>
<dt><strong><code>win_overlap</code></strong> :&ensp;<code>float</code></dt>
<dd>Window overlap fraction. The fraction of two successive windows that
overlaps. Higher overlap ensures that fewer artifact portions are going
to be missed, but is slower (default=0.66).</dd>
<dt><strong><code>max_dropout_fraction</code></strong> :&ensp;<code>float</code></dt>
<dd>Maximum fraction of windows that can be subject to signal dropouts
(e.g., sensor unplugged), used for threshold estimation (default=0.1).</dd>
<dt><strong><code>min_clean_fraction</code></strong> :&ensp;<code>float</code></dt>
<dd>Minimum fraction of windows that need to be clean, used for threshold
estimation (default=0.25).</dd>
<dt><strong><code>ab</code></strong> :&ensp;<code>2-tuple | None</code></dt>
<dd>Coefficients (A, B) of an IIR filter that is used to shape the
spectrum of the signal when calculating artifact statistics. The
output signal does not go through this filter. This is an optional way
to tune the sensitivity of the algorithm to each frequency component
of the signal. The default filter is less sensitive at alpha and beta
frequencies and more sensitive at delta (blinks) and gamma (muscle)
frequencies. Defaults to None.</dd>
<dt><strong><code>max_bad_chans</code></strong> :&ensp;<code>float</code></dt>
<dd>The maximum number or fraction of bad channels that a retained window
may still contain (more than this and it is removed). Reasonable range
is 0.05 (very clean output) to 0.3 (very lax cleaning of only coarse
artifacts) (default=0.2).</dd>
<dt><strong><code>method</code></strong> :&ensp;<code>{'riemann', 'euclid'}</code></dt>
<dd>Method to use. If riemann, use the riemannian-modified version of
ASR [2]_. Currently, only euclidean ASR is supported. Defaults to
"euclid".</dd>
</dl>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>sfreq</code></strong> :&ensp;<code>array, shape=(n_channels, filter_order)</code></dt>
<dd>Filter initial conditions.</dd>
<dt><strong><code>cutoff</code></strong> :&ensp;<code>float</code></dt>
<dd>Standard deviation cutoff for rejection.</dd>
<dt><strong><code>blocksize</code></strong> :&ensp;<code>int</code></dt>
<dd>Block size for calculating the robust data covariance and thresholds.</dd>
<dt><strong><code>win_len</code></strong> :&ensp;<code>float</code></dt>
<dd>Window length (s) that is used to check the data for artifact content.</dd>
<dt><strong><code>win_overlap</code></strong> :&ensp;<code>float</code></dt>
<dd>Window overlap fraction.</dd>
<dt><strong><code>max_dropout_fraction</code></strong> :&ensp;<code>float</code></dt>
<dd>Maximum fraction of windows that can be subject to signal dropouts.</dd>
<dt><strong><code>min_clean_fraction</code></strong> :&ensp;<code>float</code></dt>
<dd>Minimum fraction of windows.</dd>
<dt><strong><code>max_bad_chans</code></strong> :&ensp;<code>float</code></dt>
<dd>The maximum fraction of bad channels.</dd>
<dt><strong><code>method</code></strong> :&ensp;<code>{'riemann', 'euclid'}</code></dt>
<dd>Method to use.</dd>
<dt><strong><code>A</code></strong>, <strong><code>B</code></strong> :&ensp;<code>arrays</code></dt>
<dd>Coefficients of an IIR filter that is used to shape the spectrum of the
signal when calculating artifact statistics. The output signal does not
go through this filter. This is an optional way to tune the sensitivity
of the algorithm to each frequency component of the signal. The default
filter is less sensitive at alpha and beta frequencies and more
sensitive at delta (blinks) and gamma (muscle) frequencies.</dd>
<dt><strong><code>M</code></strong> :&ensp;<code>array, shape=(channels, channels)</code></dt>
<dd>The mixing matrix to fit ASR data.</dd>
<dt><strong><code>T</code></strong> :&ensp;<code>array, shape=(channels, channels)</code></dt>
<dd>The mixing matrix to fit ASR data.</dd>
</dl>
<h2 id="references">References</h2>
<p>.. [1] Kothe, C. A. E., &amp; Jung, T. P. (2016). U.S. Patent Application No.
14/895,440. <a href="https://patents.google.com/patent/US20160113587A1/en">https://patents.google.com/patent/US20160113587A1/en</a>
.. [2] Blum, S., Jacobsen, N. S. J., Bleichner, M. G., &amp; Debener, S.
(2019). A Riemannian Modification of Artifact Subspace Reconstruction
for EEG Artifact Handling. Frontiers in Human Neuroscience, 13.
<a href="https://doi.org/10.3389/fnhum.2019.00141">https://doi.org/10.3389/fnhum.2019.00141</a></p></div>
<h3>Methods</h3>
<dl>
<dt id="asrpy.asr.ASR.fit"><code class="name flex">
<span>def <span class="ident">fit</span></span>(<span>self, raw, picks='eeg', start=0, stop=None, return_clean_window=False)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fit(self, raw, picks=&#34;eeg&#34;, start=0, stop=None,
        return_clean_window=False):
    &#34;&#34;&#34;Calibration for the Artifact Subspace Reconstruction method.

    The input to this data is a multi-channel time series of calibration
    data. In typical uses the calibration data is clean resting EEG data
    of data if the fraction of artifact content is below the breakdown
    point of the robust statistics used for estimation (50% theoretical,
    ~30% practical). If the data has a proportion of more than 30-50%
    artifacts then bad time windows should be removed beforehand. This
    data is used to estimate the thresholds that are used by the ASR
    processing function to identify and remove artifact components.

    The calibration data must have been recorded for the same cap design
    from which data for cleanup will be recorded, and ideally should be
    from the same session and same subject, but it is possible to reuse
    the calibration data from a previous session and montage to the
    extent that the cap is placed in the same location (where loss in
    accuracy is more or less proportional to the mismatch in cap
    placement).

    Parameters
    ----------
    raw : instance of mne.io.Raw
        Instance of mne.io.Raw to be used for fitting the ASR.
        The calibration data should have been high-pass filtered (for
        example at 0.5Hz or 1Hz using a Butterworth IIR filter), and be
        reasonably clean not less than 30 seconds (this method is
        typically used with 1 minute or more).
    picks : str | list | slice | None
        Channels used to fit the ASR. All channels should be of the same
        type (e.g. &#34;eeg&#34;, &#34;grads&#34;). Slices and lists of integers will
        be interpreted as channel indices. In lists, channel
        name strings (e.g., [&#39;MEG0111&#39;, &#39;MEG2623&#39;] will pick the given
        channels. Note that channels in info[&#39;bads&#39;] will be included if
        their names or indices are explicitly provided. Defaults to &#34;eeg&#34;.
    start : int
        The first sample to use for fitting the data. Defaults to 0.
    stop : int | None
        The last sample to use for fitting the data. If `None`, all
        samples after `start` will be used for fitting. Defaults to None.
    return_clean_window : Bool
        If True, the method will return the variables `clean` (the cropped
         dataset which was used to fit the ASR) and `sample_mask` (a
         logical mask of which samples were included/excluded from fitting
         the ASR). Defaults to False.

    Returns
    -------
    clean : array, shape=(n_channels, n_samples)
        The cropped version of the dataset which was used to calibrate
        the ASR. This array is a result of the `clean_windows` function
        and no ASR was applied to it.
    sample_mask : boolean array, shape=(1, n_samples)
        Logical mask of the samples which were used to train the ASR.

    &#34;&#34;&#34;

    # extract the data
    X = raw.get_data(picks=picks, start=start, stop=stop)

    # Find artifact-free windows first
    clean, sample_mask = clean_windows(
        X,
        sfreq=self.sfreq,
        win_len=self.win_len,
        win_overlap=self.win_overlap,
        max_bad_chans=self.max_bad_chans,
        min_clean_fraction=self.min_clean_fraction,
        max_dropout_fraction=self.max_dropout_fraction)

    # Perform calibration
    self.M, self.T = asr_calibrate(
        clean,
        sfreq=self.sfreq,
        cutoff=self.cutoff,
        blocksize=self.blocksize,
        win_len=self.win_len,
        win_overlap=self.win_overlap,
        max_dropout_fraction=self.max_dropout_fraction,
        min_clean_fraction=self.min_clean_fraction,
        ab=(self.A, self.B),
        method=self.method)

    self._fitted = True

    # return data if required
    if return_clean_window:
        return clean, sample_mask</code></pre>
</details>
<div class="desc"><p>Calibration for the Artifact Subspace Reconstruction method.</p>
<p>The input to this data is a multi-channel time series of calibration
data. In typical uses the calibration data is clean resting EEG data
of data if the fraction of artifact content is below the breakdown
point of the robust statistics used for estimation (50% theoretical,
~30% practical). If the data has a proportion of more than 30-50%
artifacts then bad time windows should be removed beforehand. This
data is used to estimate the thresholds that are used by the ASR
processing function to identify and remove artifact components.</p>
<p>The calibration data must have been recorded for the same cap design
from which data for cleanup will be recorded, and ideally should be
from the same session and same subject, but it is possible to reuse
the calibration data from a previous session and montage to the
extent that the cap is placed in the same location (where loss in
accuracy is more or less proportional to the mismatch in cap
placement).</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>raw</code></strong> :&ensp;<code>instance</code> of <code>mne.io.Raw</code></dt>
<dd>Instance of mne.io.Raw to be used for fitting the ASR.
The calibration data should have been high-pass filtered (for
example at 0.5Hz or 1Hz using a Butterworth IIR filter), and be
reasonably clean not less than 30 seconds (this method is
typically used with 1 minute or more).</dd>
<dt><strong><code>picks</code></strong> :&ensp;<code>str | list | slice | None</code></dt>
<dd>Channels used to fit the ASR. All channels should be of the same
type (e.g. "eeg", "grads"). Slices and lists of integers will
be interpreted as channel indices. In lists, channel
name strings (e.g., ['MEG0111', 'MEG2623'] will pick the given
channels. Note that channels in info['bads'] will be included if
their names or indices are explicitly provided. Defaults to "eeg".</dd>
<dt><strong><code>start</code></strong> :&ensp;<code>int</code></dt>
<dd>The first sample to use for fitting the data. Defaults to 0.</dd>
<dt><strong><code>stop</code></strong> :&ensp;<code>int | None</code></dt>
<dd>The last sample to use for fitting the data. If <code>None</code>, all
samples after <code>start</code> will be used for fitting. Defaults to None.</dd>
<dt><strong><code>return_clean_window</code></strong> :&ensp;<code>Bool</code></dt>
<dd>If True, the method will return the variables <code>clean</code> (the cropped
dataset which was used to fit the ASR) and <code>sample_mask</code> (a
logical mask of which samples were included/excluded from fitting
the ASR). Defaults to False.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>clean</code></strong> :&ensp;<code>array, shape=(n_channels, n_samples)</code></dt>
<dd>The cropped version of the dataset which was used to calibrate
the ASR. This array is a result of the <code><a title="asrpy.asr.clean_windows" href="#asrpy.asr.clean_windows">clean_windows()</a></code> function
and no ASR was applied to it.</dd>
<dt><strong><code>sample_mask</code></strong> :&ensp;<code>boolean array, shape=(1, n_samples)</code></dt>
<dd>Logical mask of the samples which were used to train the ASR.</dd>
</dl></div>
</dd>
<dt id="asrpy.asr.ASR.transform"><code class="name flex">
<span>def <span class="ident">transform</span></span>(<span>self,<br>raw,<br>picks='eeg',<br>lookahead=0.25,<br>stepsize=32,<br>maxdims=0.66,<br>return_states=False,<br>mem_splits=3)</span>
</code></dt>
<dd>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def transform(self, raw, picks=&#34;eeg&#34;, lookahead=0.25, stepsize=32,
              maxdims=0.66, return_states=False, mem_splits=3):
    &#34;&#34;&#34;Apply Artifact Subspace Reconstruction.

    Parameters
    ----------
    raw : instance of mne.io.Raw
        Instance of mne.io.Raw to be transformed by the ASR.
    picks : str | list | slice | None
        Channels to be transformed by the ASR. Should be the same set of
        channels as used by `ASR.fit()`. All channels should be of the
        same type (e.g. &#34;eeg&#34;, &#34;grads&#34;). Slices and lists of integers will
        be interpreted as channel indices. In lists, channel
        name strings (e.g., [&#39;MEG0111&#39;, &#39;MEG2623&#39;] will pick the given
        channels. Note that channels in info[&#39;bads&#39;] will be included if
        their names or indices are explicitly provided. Defaults to &#34;eeg&#34;.
    lookahead : float
        Amount of look-ahead that the algorithm should use (in seconds).
        This value should be between 0 (no lookahead) and WindowLength/2
        (optimal lookahead). The recommended value is WindowLength/2.
        Default: 0.25

    Note: Other than in `asr_process`, the signal will be readjusted
        to eliminate any temporal jitter and automatically readjust it to
        the correct time points. Zero-padding will be applied to the last
        `lookahead` portion of the data, possibly resulting in inaccuracies
        for the final `lookahead` seconds of the recording.
    stepsize : int
        The steps in which the algorithm will be updated. The larger this
        is, the faster the algorithm will be. The value must not be larger
        than WindowLength * SamplingRate. The minimum value is 1 (update
        for every sample) while a good value would be sfreq//3. Note that
        an update is always performed also on the first and last sample of
        the data chunk. Default: 32
    max_dims : float, int
        Maximum dimensionality of artifacts to remove. This parameter
        denotes the maximum number of dimensions which can be removed from
        each segment. If larger than 1, `int(max_dims)` will denote the
        maximum number of dimensions removed from the data. If smaller
        than 1, `max_dims` describes a fraction of total dimensions.
        Defaults to 0.66.
    return_states : bool
        If True, returns a dict including the updated states {&#34;M&#34;:M,
        &#34;T&#34;:T, &#34;R&#34;:R, &#34;Zi&#34;:Zi, &#34;cov&#34;:cov, &#34;carry&#34;:carry}. Defaults to
        False.
    mem_splits : int
        Split the array in `mem_splits` segments to save memory.

    Returns
    -------
    out : array, shape=(n_channels, n_samples)
        Filtered data.

    &#34;&#34;&#34;
    # extract the data
    X = raw.get_data(picks=picks)

    # add lookahead padding at the end
    lookahead_samples = int(self.sfreq * lookahead)
    X = np.concatenate([X,
                        np.zeros([X.shape[0], lookahead_samples])],
                       axis=1)

    # apply ASR
    X = asr_process(X, self.sfreq, self.M, self.T, self.win_len,
                    lookahead, stepsize, maxdims, (self.A, self.B),
                    self.R, self.Zi, self.cov, self.carry,
                    return_states, self.method, mem_splits)

    # remove lookahead portion from start
    X = X[:, lookahead_samples:]

    # Return a modifier raw instance
    raw = raw.copy()
    raw.apply_function(lambda x: X, picks=picks,
                       channel_wise=False)
    return raw</code></pre>
</details>
<div class="desc"><p>Apply Artifact Subspace Reconstruction.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>raw</code></strong> :&ensp;<code>instance</code> of <code>mne.io.Raw</code></dt>
<dd>Instance of mne.io.Raw to be transformed by the ASR.</dd>
<dt><strong><code>picks</code></strong> :&ensp;<code>str | list | slice | None</code></dt>
<dd>Channels to be transformed by the ASR. Should be the same set of
channels as used by <code><a title="asrpy.asr.ASR.fit" href="#asrpy.asr.ASR.fit">ASR.fit()</a></code>. All channels should be of the
same type (e.g. "eeg", "grads"). Slices and lists of integers will
be interpreted as channel indices. In lists, channel
name strings (e.g., ['MEG0111', 'MEG2623'] will pick the given
channels. Note that channels in info['bads'] will be included if
their names or indices are explicitly provided. Defaults to "eeg".</dd>
<dt><strong><code>lookahead</code></strong> :&ensp;<code>float</code></dt>
<dd>Amount of look-ahead that the algorithm should use (in seconds).
This value should be between 0 (no lookahead) and WindowLength/2
(optimal lookahead). The recommended value is WindowLength/2.
Default: 0.25</dd>
<dt><strong><code>Note</code></strong> :&ensp;<code>Other than in </code>asr_process<code>, the signal will be readjusted</code></dt>
<dd>to eliminate any temporal jitter and automatically readjust it to
the correct time points. Zero-padding will be applied to the last
<code>lookahead</code> portion of the data, possibly resulting in inaccuracies
for the final <code>lookahead</code> seconds of the recording.</dd>
<dt><strong><code>stepsize</code></strong> :&ensp;<code>int</code></dt>
<dd>The steps in which the algorithm will be updated. The larger this
is, the faster the algorithm will be. The value must not be larger
than WindowLength * SamplingRate. The minimum value is 1 (update
for every sample) while a good value would be sfreq//3. Note that
an update is always performed also on the first and last sample of
the data chunk. Default: 32</dd>
<dt><strong><code>max_dims</code></strong> :&ensp;<code>float, int</code></dt>
<dd>Maximum dimensionality of artifacts to remove. This parameter
denotes the maximum number of dimensions which can be removed from
each segment. If larger than 1, <code>int(max_dims)</code> will denote the
maximum number of dimensions removed from the data. If smaller
than 1, <code>max_dims</code> describes a fraction of total dimensions.
Defaults to 0.66.</dd>
<dt><strong><code>return_states</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, returns a dict including the updated states {"M":M,
"T":T, "R":R, "Zi":Zi, "cov":cov, "carry":carry}. Defaults to
False.</dd>
<dt><strong><code>mem_splits</code></strong> :&ensp;<code>int</code></dt>
<dd>Split the array in <code>mem_splits</code> segments to save memory.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>out</code></strong> :&ensp;<code>array, shape=(n_channels, n_samples)</code></dt>
<dd>Filtered data.</dd>
</dl></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="asrpy" href="index.html">asrpy</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="asrpy.asr.asr_calibrate" href="#asrpy.asr.asr_calibrate">asr_calibrate</a></code></li>
<li><code><a title="asrpy.asr.asr_process" href="#asrpy.asr.asr_process">asr_process</a></code></li>
<li><code><a title="asrpy.asr.clean_windows" href="#asrpy.asr.clean_windows">clean_windows</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="asrpy.asr.ASR" href="#asrpy.asr.ASR">ASR</a></code></h4>
<ul class="">
<li><code><a title="asrpy.asr.ASR.fit" href="#asrpy.asr.ASR.fit">fit</a></code></li>
<li><code><a title="asrpy.asr.ASR.transform" href="#asrpy.asr.ASR.transform">transform</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.11.6</a>.</p>
</footer>
</body>
</html>
